{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model with PyTorch\n",
    "\n",
    "> This notebook is a simplified version of our workflow. It exposes the basic details of the traning and evaluation loop more explicitly, but does not offer advanced features like early stopping, mini-batches or validation. Use the `*-lightning` version for those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Run `python run_notebook.py --help` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is the template file (and not a copy) and you are introducing changes,\n",
    "# update VERSION with the current date (YYYY.MM.DD)\n",
    "VERSION = \"2021.04.09\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úè Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# TEMPLATE VALUES -- these are overriden (see below if executed) by papermill using a YAML or Python file as input\n",
    "\n",
    "# DATA -- Glob paths must be relative to the root of the repository: REPO / features\n",
    "NPZ_FILES = [\n",
    "    \"path/to/*.npz\",\n",
    "]\n",
    "\n",
    "# Model -- specified with the full import path to the class object\n",
    "MODEL_CLS = \"kinoml.ml.torch_models.NeuralNetworkRegression\"\n",
    "MODEL_KWARGS = {\"hidden_size\": 350}  # input_shape is defined dynamically during training\n",
    "WITH_OBSERVATION_MODEL = True\n",
    "\n",
    "# Adam\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1e-7\n",
    "BETAS = 0.9, 0.999\n",
    "\n",
    "# Trainer\n",
    "MAX_EPOCHS = 50\n",
    "N_SPLITS = 5\n",
    "SHUFFLE_FOLDS = False\n",
    "VALIDATION = False  # TODO: VALIDATION=True is not implemented yet!\n",
    "MIN_ITEMS_PER_DATASET = 50  # skip datasets if len(data) < N\n",
    "\n",
    "# Bootstrapping\n",
    "N_BOOTSTRAPS = 1\n",
    "BOOTSTRAP_SAMPLE_RATIO = 1\n",
    "\n",
    "# Output\n",
    "VERBOSE = False\n",
    "\n",
    "## IGNORE THIS ONE\n",
    "HERE = _dh[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö† From here on, you should _not_ need to modify anything else ü§û\n",
    "\n",
    "---\n",
    "\n",
    "Define key paths for data and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "HERE = Path(HERE)\n",
    "\n",
    "for parent in HERE.parents:\n",
    "    if next(parent.glob(\".github/\"), None):\n",
    "        REPO = parent\n",
    "        break\n",
    "\n",
    "FEATURES_STORE = REPO / \"features\"\n",
    "        \n",
    "OUT = HERE / \"_output\" / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"This notebook:           HERE = ~/{HERE.relative_to(Path.home())}\")\n",
    "print(f\"This repo:               REPO = ~/{REPO.relative_to(Path.home())}\")\n",
    "print(f\"Features:      FEATURES_STORE = ~/{FEATURES_STORE.relative_to(Path.home())}\")\n",
    "print(f\"Outputs in:               OUT = ~/{OUT.relative_to(Path.home())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasty trick: save all-caps local variables (CONSTANTS working as hyperparameters) so far in a dict to save it later\n",
    "_hparams = {key: value for key, value in locals().items() if key.upper() == key and not key.startswith((\"_\", \"OE_\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make all datasets use the same kinase identifiers\n",
    "ONE_KINASE = {\n",
    "    \"ChEMBLDatasetProvider\": \"P35968\",\n",
    "    \"PKIS2DatasetProvider\": \"ABL2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from kinoml.utils import seed_everything, import_object\n",
    "from kinoml.core import measurements as measurement_types\n",
    "from kinoml.datasets.torch_datasets import XyNpzTorchDataset\n",
    "from kinoml.core.measurements import null_observation_model\n",
    "\n",
    "# Fix the seed for reproducible random splits -- otherwise we get mixed train/test groups every time, biasing the model evaluation\n",
    "seed_everything();\n",
    "print(\"Run started at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load featurized data and create observation models\n",
    "\n",
    "We assume this path structure: `$REPO/features/_output/<FEATURIZATION>/<DATASET>/<GROUP>.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = []\n",
    "MEASUREMENT_TYPES = set()\n",
    "KINASES = set()\n",
    "FEATURIZATIONS = set()\n",
    "for glob in NPZ_FILES:\n",
    "    npzs = list(FEATURES_STORE.glob(glob))\n",
    "    if not npzs:\n",
    "        warn(f\"‚ö† NPZ glob `{glob}` did not match any files!\")\n",
    "        continue\n",
    "        \n",
    "    for npz in npzs:\n",
    "        kinase, measurement_type = npz.stem.split(\"__\")\n",
    "        dataset = npz.parent.name\n",
    "        featurization = npz.parents[1].name\n",
    "        \n",
    "        MEASUREMENT_TYPES.add(measurement_type)\n",
    "        KINASES.add(kinase)\n",
    "        FEATURIZATIONS.add(featurization)\n",
    "        \n",
    "        ds = XyNpzTorchDataset(npz)\n",
    "        ds.metadata = {\n",
    "            \"kinase\": kinase,\n",
    "            \"measurement_type\": measurement_type,\n",
    "            \"dataset\": dataset,\n",
    "            \"featurization\": featurization\n",
    "        }\n",
    "        DATASETS.append(ds)\n",
    "        if not VALIDATION:\n",
    "            ds.indices[\"test\"] = np.concatenate([ds.indices[\"test\"], ds.indices[\"val\"]])\n",
    "            ds.indices[\"val\"] = np.array([])\n",
    "\n",
    "if not DATASETS:\n",
    "    raise ValueError(\"Provided `NPZ_FILES` did not result in any valid datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observed...\")\n",
    "print(\" - Measurement types:\", len(MEASUREMENT_TYPES), \"-->\", *MEASUREMENT_TYPES)\n",
    "print(\" - Kinases:\", len(KINASES), \"-->\", *KINASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check X duplication\n",
    "\n",
    "There's a chance we have several measurements per ligand, or, depending on the featurization scheme, even hash collisions... Let's quantify the amount of input tensor duplication we are facing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mtype in MEASUREMENT_TYPES:\n",
    "    display(Markdown(f\"#### {mtype}\"))\n",
    "    unique = {}\n",
    "    for ds in DATASETS:\n",
    "        if ds.metadata[\"measurement_type\"] == mtype:\n",
    "            all_ = ds.data_X.shape[0]\n",
    "            unique_ = np.unique(ds.data_X, axis=0).shape[0]\n",
    "            unique[ds.metadata[\"kinase\"]] = {\"all\": all_, \"unique\": unique_}\n",
    "    df = pd.DataFrame.from_dict(unique).T\n",
    "    df[\"uniqueness\"] = df[\"unique\"] / df[\"all\"]\n",
    "    # This is how you highlight rows in pandas!\n",
    "    df = df.describe().style.apply(lambda x: ['font-weight: bold' for v in x], subset=pd.IndexSlice[[\"mean\", \"std\"], :])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data-dependent objects, we can start with the model-specific definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.ml.lightning_modules import KFold3Way, KFold\n",
    "from IPython.display import Markdown\n",
    "from tqdm.auto import trange, tqdm\n",
    "from kinoml.ml.torch_models import NeuralNetworkRegression\n",
    "from ipywidgets import HBox, VBox, Output, HTML\n",
    "from kinoml.analysis.plots import predicted_vs_observed, performance\n",
    "from kinoml.utils import fill_until_next_multiple\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "if VALIDATION:\n",
    "    kfold = KFold3Way(n_splits=N_SPLITS, shuffle=SHUFFLE_FOLDS)\n",
    "    ttypes = [\"train\", \"val\", \"test\"]\n",
    "else:\n",
    "    kfold = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE_FOLDS)\n",
    "    ttypes = [\"train\", \"test\"]\n",
    "\n",
    "ModelCls = import_object(MODEL_CLS)\n",
    "    \n",
    "kinase_metrics = defaultdict(dict)\n",
    "for dataset in tqdm(DATASETS):\n",
    "    kinase = dataset.metadata[\"kinase\"]\n",
    "    mtype = dataset.metadata[\"measurement_type\"]\n",
    "    if dataset.data_X.shape[0] < MIN_ITEMS_PER_DATASET:\n",
    "        warn(f\"Ignoring {kinase} because it has less than {MIN_ITEMS_PER_DATASET} entries for type {mtype}\")\n",
    "        continue\n",
    "            \n",
    "    if VERBOSE:\n",
    "        display(Markdown(f\"#### {mtype}\"))\n",
    "\n",
    "    mtype_class = getattr(measurement_types, mtype)\n",
    "    obs_model = mtype_class.observation_model(backend=\"pytorch\")\n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    for fold_index, splits in enumerate(kfold.split(dataset.data_X, dataset.data_y)):\n",
    "        if VALIDATION:\n",
    "            train_indices, val_indices, test_indices = splits\n",
    "        else:\n",
    "            train_indices, test_indices = splits\n",
    "\n",
    "        if VERBOSE:\n",
    "            display(Markdown(f\"##### Fold {fold_index}\"))\n",
    "\n",
    "        ####\n",
    "        # TRAIN\n",
    "        ####\n",
    "        x_train = dataset.data_X[train_indices].float()\n",
    "        x_test = dataset.data_X[test_indices].float()\n",
    "        y_train = dataset.data_y[train_indices]\n",
    "        y_test = dataset.data_y[test_indices]\n",
    "\n",
    "        if VALIDATION:\n",
    "            x_val = dataset.data_X[val_indices].float()\n",
    "            y_val = dataset.data_y[val_indices]\n",
    "        \n",
    "        input_shape = ModelCls.estimate_input_shape(x_train)\n",
    "        nn_model = ModelCls(input_shape=input_shape, **MODEL_KWARGS)\n",
    "        nn_model.train(True)\n",
    "\n",
    "        optimizer = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE, eps=EPSILON, betas=BETAS)\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "\n",
    "        if VERBOSE:\n",
    "            range_epochs = trange(MAX_EPOCHS, desc=\"Epochs (+ featurization...)\")\n",
    "        else:\n",
    "            range_epochs = range(MAX_EPOCHS)\n",
    "        for epoch in range_epochs:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = nn_model(x_train)\n",
    "            if WITH_OBSERVATION_MODEL:\n",
    "                prediction = obs_model(prediction)\n",
    "\n",
    "            prediction = prediction.view_as(y_train)\n",
    "\n",
    "            loss = loss_function(prediction, y_train)\n",
    "            if VERBOSE:\n",
    "                range_epochs.set_description(f\"Epochs (loss={loss.item():.2e})\")\n",
    "\n",
    "            if VALIDATION:\n",
    "                warn(\"Validation step not implemented yet\")\n",
    "\n",
    "\n",
    "            # Gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer.step()\n",
    "        \n",
    "        ###\n",
    "        # Save model's state -- you will still need to instantiate the model class!\n",
    "        # Possibly using something like:\n",
    "        # model = import_object(MODEL_CLS)(**MODEL_KWARGS)\n",
    "        # model.load_state_dict(torch.load(\"state_dict.pt\"))\n",
    "        ###\n",
    "        torch.save(nn_model.state_dict(), OUT / f\"state_dict_{kinase}_{mtype}_fold{fold_index}.pt\")\n",
    "        \n",
    "        ####\n",
    "        # EVAL\n",
    "        ####\n",
    "        nn_model.eval()\n",
    "        outputs = []\n",
    "        for ttype in ttypes:\n",
    "            output = Output()\n",
    "            with output:\n",
    "                title = f\"fold={fold_index}, {ttype}={locals()[f'{ttype}_indices'].shape[0]}\"\n",
    "                print(title)\n",
    "                print(\"-\"*(len(title)))\n",
    "\n",
    "                observed = locals()[f\"y_{ttype}\"]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predicted = nn_model(locals()[f\"x_{ttype}\"])\n",
    "                    if WITH_OBSERVATION_MODEL:\n",
    "                        predicted = obs_model(predicted)\n",
    "\n",
    "                predicted = predicted.view_as(observed).detach().numpy()\n",
    "                observed = observed.detach().numpy()\n",
    "                these_metrics = performance(predicted, observed, n_boot=N_BOOTSTRAPS, sample_ratio=BOOTSTRAP_SAMPLE_RATIO)\n",
    "                metrics[ttype].append(these_metrics)\n",
    "                if VERBOSE:\n",
    "                    display(predicted_vs_observed(predicted, observed, mtype_class, with_metrics=False))\n",
    "\n",
    "            outputs.append(output)\n",
    "        if VERBOSE:\n",
    "            display(HBox(outputs))\n",
    "\n",
    "    # Average performances\n",
    "\n",
    "    average = defaultdict(dict)\n",
    "    for key in metrics[\"test\"][0]:\n",
    "        for label in ttypes:\n",
    "            # this zero here ---v is super important! we only want the mean of the means!\n",
    "            values =  [fold[key][0] for fold in metrics[label]]\n",
    "            average[label][key] = {\n",
    "                \"mean\": np.mean(values),\n",
    "                \"std\": np.std(values)\n",
    "            }\n",
    "    if VERBOSE:\n",
    "        for label in ttypes:    \n",
    "            display(HTML(f\"Bootstrapped average across folds ({label}):\"))\n",
    "            display(pd.DataFrame.from_dict(average[label]))\n",
    "    kinase_metrics[kinase][mtype] = average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "`kinase_metrics` is a nested dictionary with these dimensions:\n",
    "\n",
    "- kinase name\n",
    "- measurement type\n",
    "- metric\n",
    "- mean & standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Configuration \n",
    "\n",
    "```json\n",
    "{json.dumps(_hparams, default=str, indent=2)}\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "if VERBOSE:\n",
    "    display(Markdown(f\"\"\"\n",
    "\n",
    "    ### Kinase metrics\n",
    "\n",
    "    ```json\n",
    "    {json.dumps(kinase_metrics, default=str, indent=2)}\n",
    "    ```\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mtype in MEASUREMENT_TYPES:\n",
    "    display(Markdown(f\"#### {mtype}\"))\n",
    "\n",
    "    dict_of_flattened_metrics = {}\n",
    "    for kinase_name, measurement_type_dict in sorted(kinase_metrics.items(), key=lambda kv: kv[0].lower()):\n",
    "        flattened_metrics = {}\n",
    "        for train_test_key, train_test_dict in measurement_type_dict.get(mtype, {}).items():\n",
    "            for metric_key, mean_std_dict in train_test_dict.items():\n",
    "                for mean_std_key, value in mean_std_dict.items():\n",
    "                    flattened_metrics[f\"{train_test_key}_{metric_key}_{mean_std_key}\"] = (value,)\n",
    "        if flattened_metrics:\n",
    "            dict_of_flattened_metrics[kinase_name] = pd.DataFrame.from_dict(flattened_metrics)\n",
    "    \n",
    "    if not dict_of_flattened_metrics:\n",
    "        continue\n",
    "    \n",
    "    df = pd.concat(dict_of_flattened_metrics)\n",
    "    df.index = [index[0] for index in df.index]\n",
    "    with pd.option_context(\"display.float_format\", \"{:.3f}\".format, \"display.max_rows\", len(df)):\n",
    "        display(df.style.background_gradient(subset=[\"train_r2_mean\", \"test_r2_mean\"], low=0, high=1, vmin=0, vmax=1))\n",
    "        display(df.describe()[[\"train_r2_mean\", \"train_r2_std\", \"test_r2_mean\", \"test_r2_std\"]].describe().style.apply(lambda x: ['font-weight: bold' for v in x], subset=pd.IndexSlice[[\"mean\", \"std\"], :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run finished at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save reports to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.utils import watermark\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df.to_csv(OUT / \"performance.csv\")\n",
    "\n",
    "with open(OUT / \"performance.json\", \"w\") as f:\n",
    "    json.dump(kinase_metrics, f, default=str, indent=2)\n",
    "    \n",
    "with open(OUT/ \"watermark.txt\", \"w\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "with open(OUT / \"hparams.json\", \"w\") as f:\n",
    "    json.dump(_hparams, f, default=str, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
