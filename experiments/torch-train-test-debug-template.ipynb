{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model with PyTorch\n",
    "\n",
    "This notebooks trains a single model using only PyTorch.\n",
    "\n",
    "1. Tensors are loaded from Parquet files generated in the `features/` pipeline. Each Parquet becomes a Torch Dataset sublass.\n",
    "2. Random splits are applied for train/test/(val).\n",
    "3. It will train a single for model for a number of epochs across all datasets: epoch> dataloader> minibatch.\n",
    "4. The loss is computed through the `loss_adapter` method in each measurement_type.\n",
    "5. If validation is enabled, early stopping and LR schedulers are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Run `python run_notebook.py --help` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is the template file (and not a copy) and you are introducing changes,\n",
    "# update VERSION with the current date (YYYY.MM.DD)\n",
    "VERSION = \"2021.05.19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úè Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# TEMPLATE VALUES -- these are overriden (see below if executed) by papermill using a YAML or Python file as input\n",
    "\n",
    "# DATA -- Glob paths must be relative to the root of the repository: REPO / features\n",
    "PARQUET_FILES = [\n",
    "    \"path/to/*.parquet\",\n",
    "]\n",
    "\n",
    "# Model -- specified with the full import path to the class object\n",
    "MODEL_CLS = \"kinoml.ml.torch_models.NeuralNetworkRegression\"\n",
    "MODEL_KWARGS = {\"hidden_size\": 350}  # input_shape is defined dynamically during training\n",
    "WITH_OBSERVATION_MODEL = True\n",
    "\n",
    "# Adam\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1e-7\n",
    "BETAS = 0.9, 0.999\n",
    "\n",
    "# Trainer\n",
    "MAX_EPOCHS = 50\n",
    "VALIDATION = False\n",
    "\n",
    "# DATALOADER\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_TEST_SPLIT = 0.2\n",
    "SHUFFLE_SPLITS = False\n",
    "\n",
    "# Bootstrapping\n",
    "N_BOOTSTRAPS = 1\n",
    "BOOTSTRAP_SAMPLE_RATIO = 1\n",
    "\n",
    "# Output\n",
    "VERBOSE = False\n",
    "\n",
    "## IGNORE THIS ONE\n",
    "HERE = _dh[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö† From here on, you should _not_ need to modify anything else ü§û\n",
    "\n",
    "---\n",
    "\n",
    "Define key paths for data and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "HERE = Path(HERE)\n",
    "\n",
    "for parent in HERE.parents:\n",
    "    if next(parent.glob(\".github/\"), None):\n",
    "        REPO = parent\n",
    "        break\n",
    "\n",
    "FEATURES_STORE = REPO / \"features\"\n",
    "        \n",
    "OUT = HERE / \"_output\" / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"This notebook:           HERE = {HERE}\")\n",
    "print(f\"This repo:               REPO = {REPO}\")\n",
    "print(f\"Features:      FEATURES_STORE = {FEATURES_STORE}\")\n",
    "print(f\"Outputs in:               OUT = {OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasty trick: save all-caps local variables (CONSTANTS working as hyperparameters) so far in a dict to save it later\n",
    "_hparams = {key: value for key, value in locals().items() if key.upper() == key and not key.startswith((\"_\", \"OE_\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "from kinoml.ml.torch_loops import LRScheduler, EarlyStopping\n",
    "from kinoml.utils import seed_everything, import_object\n",
    "from kinoml.core import measurements as measurement_types\n",
    "from kinoml.datasets.torch_datasets import AwkwardArrayDataset\n",
    "from kinoml.core.measurements import null_observation_model\n",
    "from kinoml.analysis.metrics import performance\n",
    "from kinoml.analysis.plots import predicted_vs_observed\n",
    "\n",
    "# Fix the seed for reproducible random splits -- otherwise we get mixed train/test groups every time, biasing the model evaluation\n",
    "seed_everything();\n",
    "print(\"Run started at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load featurized data and create observation models\n",
    "\n",
    "We assume this path structure: `$REPO/features/_output/<FEATURIZATION>/<DATASET>/<MEASUREMENT_TYPE>.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = []\n",
    "MEASUREMENT_TYPES = set()\n",
    "for glob in PARQUET_FILES:\n",
    "    parquets = list(FEATURES_STORE.glob(glob))\n",
    "    if not parquets:\n",
    "        warn(f\"‚ö† Parquet glob `{glob}` did not match any files!\")\n",
    "        continue\n",
    "        \n",
    "    for parquet in parquets:\n",
    "        measurement_type = parquet.stem\n",
    "        dataset = parquet.parent.name\n",
    "        \n",
    "        ds = AwkwardArrayDataset.from_parquet(parquet)\n",
    "        ds.metadata = {\n",
    "            \"dataset\": dataset,\n",
    "            \"measurement_type\": measurement_type,\n",
    "        }\n",
    "        DATASETS.append(ds)\n",
    "        MEASUREMENT_TYPES.add(measurement_type)\n",
    "\n",
    "if not DATASETS:\n",
    "    raise ValueError(\"Provided `PARQUET_FILES` did not result in any valid datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data-dependent objects, we can start with the model-specific definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(DATASETS)} datasets with a total of {sum(len(d) for d in DATASETS)} measurements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare splits\n",
    "\n",
    "Create train / test / validation subsets. Here we implement a random split, but it can take external indices if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "for dataset in DATASETS:\n",
    "    key = dataset.metadata[\"measurement_type\"]\n",
    "    \n",
    "    # Generate random indices in situ\n",
    "    # If you need to provide indices from another source, \n",
    "    # replace this block to provide train_indices, test_indices\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(TRAIN_TEST_SPLIT * dataset_size))\n",
    "    \n",
    "    if SHUFFLE_SPLITS :\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, test_indices = indices[split:], indices[split:]\n",
    "    if VALIDATION:\n",
    "        split2 = int(np.floor(len(test_indices) / 2))\n",
    "        test_indices, val_indices = test_indices[:split2], test_indices[split2:]\n",
    "    # End of indices creation\n",
    "    \n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    dataloaders[key] = {\n",
    "        \"train\": DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler),\n",
    "        \"test\": DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler),\n",
    "    }\n",
    "    \n",
    "    if VALIDATION:\n",
    "        val_sampler = SubsetRandomSampler(val_indices)\n",
    "        dataloaders[key][\"val\"] = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCls = import_object(MODEL_CLS)\n",
    "\n",
    "# Note that we assume all dataloaders will provide the\n",
    "# same kind of input shape, so we onlt test on one\n",
    "if ModelCls.needs_input_shape:\n",
    "    a_dataloader = dataloaders[next(iter(dataloaders.keys()))][\"train\"]\n",
    "    x_sample, _ = next(iter(a_dataloader))\n",
    "    MODEL_KWARGS[\"input_shape\"] = ModelCls.estimate_input_shape(x_sample)\n",
    "\n",
    "nn_model = ModelCls(**MODEL_KWARGS)\n",
    "\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE, eps=EPSILON, betas=BETAS)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "if VALIDATION:\n",
    "    lr_scheduler = LRScheduler(optimizer)\n",
    "    early_stopping = EarlyStopping()\n",
    "\n",
    "train_loss_timeseries = []\n",
    "val_loss_timeseries = []\n",
    "\n",
    "range_epochs = trange(MAX_EPOCHS, desc=\"Epochs\")\n",
    "for epoch in range_epochs:\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    for key, loader in tqdm(dataloaders.items(), desc=\"Datasets\", leave=False):\n",
    "        mtype_class = import_object(f\"kinoml.core.measurements.{key}\")\n",
    "        loss_adapter = mtype_class.loss_adapter(backend=\"pytorch\")\n",
    "        \n",
    "        # TRAIN\n",
    "        nn_model.train()\n",
    "        for x, y in tqdm(loader[\"train\"], desc=\"Minibatches\", leave=False):\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Obtain model prediction given model input\n",
    "            prediction = nn_model(x).view_as(y)\n",
    "            # apply observation model\n",
    "            loss = loss_adapter(prediction.view_as(y), y, loss_function)\n",
    "            # Pred. must match y shape!    ^^^^^^^^^^\n",
    "            # Obtain loss for the predicted output\n",
    "            train_loss += loss.item()\n",
    "            # Gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        # VALIDATE\n",
    "        if VALIDATION:\n",
    "            nn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x, y in tqdm(loader[\"val\"], desc=\"Minibatches\", leave=False):\n",
    "                    prediction = nn_model(x).view_as(y)\n",
    "                    loss = loss_adapter(prediction.view_as(y), y, loss_function)\n",
    "                    val_loss += loss.item()\n",
    "                    range_epochs.set_description(f\"Epochs (Avg. val. loss={val_loss / (epoch + 1):.2e})\")\n",
    "    \n",
    "    # LOG LOSSES\n",
    "    train_loss_timeseries.append(train_loss)\n",
    "    \n",
    "    if VALIDATION:\n",
    "        val_loss_timeseries.append(val_loss)\n",
    "\n",
    "        # Adjust training if needed\n",
    "        lr_scheduler(val_loss)\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "nn_model.train(False)\n",
    "for key, loader in dataloaders.items():\n",
    "    metrics[key] = {}\n",
    "    display(Markdown(f\"#### {key}\"))\n",
    "    for ttype, dataloader in loader.items():\n",
    "        display(Markdown(f\"##### {ttype}\"))\n",
    "        mtype = import_object(f\"kinoml.core.measurements.{key}\")\n",
    "        obs_model = mtype.observation_model(backend=\"pytorch\")\n",
    "        x, y = dataloader.dataset[dataloader.batch_sampler.sampler.indices]\n",
    "        prediction = obs_model(nn_model(x).view_as(y).detach().numpy())\n",
    "\n",
    "        perf_data = performance(prediction, y, verbose=False)\n",
    "        metrics[key][ttype] = {}\n",
    "        for perfkey, values in perf_data.items():\n",
    "            metrics[key][ttype][perfkey] = {\"mean\": values[0], \"std\": values[1]}\n",
    "        display(predicted_vs_observed(prediction, y, mtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "`kinase_metrics` is a nested dictionary with these dimensions:\n",
    "\n",
    "- measurement type\n",
    "- metric\n",
    "- mean & standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Configuration \n",
    "\n",
    "```json\n",
    "{json.dumps(_hparams, default=str, indent=2)}\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "if VERBOSE:\n",
    "    display(Markdown(\n",
    "f\"\"\"\n",
    "### Kinase metrics\n",
    "\n",
    "```json\n",
    "{json.dumps(metrics, default=str, indent=2)}\n",
    "```\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mtype_name in MEASUREMENT_TYPES:\n",
    "    mtype_metrics = metrics.get(mtype_name)\n",
    "    if not mtype_metrics:\n",
    "        continue\n",
    "        \n",
    "    display(Markdown(f\"#### {mtype_name}\"))\n",
    "    \n",
    "    # flatten dict a bit: from dict[\"test\"][\"r2\"][\"mean\"] to dict[\"test\"][\"r2_mean\"]\n",
    "    flattened = {}\n",
    "    for ttype, scores in mtype_metrics.items():\n",
    "        flattened[ttype] = {}\n",
    "        for score, stats in scores.items():\n",
    "            for stat, value in stats.items():\n",
    "                flattened[ttype][f\"{score}_{stat}\"] = value\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(flattened, orient=\"index\")\n",
    "    with pd.option_context(\"display.float_format\", \"{:.3f}\".format, \"display.max_rows\", len(df)):\n",
    "        display(\n",
    "            df.style.background_gradient(subset=[\"r2_mean\"], low=0, high=1, vmin=0, vmax=1)\n",
    "              .apply(lambda x: ['font-weight: bold' for v in x], subset=[\"r2_mean\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run finished at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save reports to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.utils import watermark\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(OUT / \"performance.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, default=str, indent=2)\n",
    "    \n",
    "with open(OUT/ \"watermark.txt\", \"w\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "with open(OUT / \"hparams.json\", \"w\") as f:\n",
    "    json.dump(_hparams, f, default=str, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
