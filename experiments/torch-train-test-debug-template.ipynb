{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model with PyTorch\n",
    "\n",
    "> This notebook is a simplified version of our workflow. It exposes the basic details of the traning and evaluation loop more explicitly, but does not offer advanced features like early stopping, mini-batches or validation. Use the `*-lightning` version for those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Run `python run_notebook.py --help` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is the template file (and not a copy) and you are introducing changes,\n",
    "# update VERSION with the current date (YYYY.MM.DD)\n",
    "VERSION = \"2021.05.18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úè Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# TEMPLATE VALUES -- these are overriden (see below if executed) by papermill using a YAML or Python file as input\n",
    "\n",
    "# DATA -- Glob paths must be relative to the root of the repository: REPO / features\n",
    "PARQUET_FILES = [\n",
    "    \"path/to/*.parquet\",\n",
    "]\n",
    "\n",
    "# Model -- specified with the full import path to the class object\n",
    "MODEL_CLS = \"kinoml.ml.torch_models.NeuralNetworkRegression\"\n",
    "MODEL_KWARGS = {\"hidden_size\": 350}  # input_shape is defined dynamically during training\n",
    "WITH_OBSERVATION_MODEL = True\n",
    "\n",
    "# Adam\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1e-7\n",
    "BETAS = 0.9, 0.999\n",
    "\n",
    "# Trainer\n",
    "MAX_EPOCHS = 50\n",
    "N_SPLITS = 5\n",
    "SHUFFLE_FOLDS = False\n",
    "VALIDATION = False  # TODO: VALIDATION=True is not implemented yet!\n",
    "MIN_ITEMS_PER_DATASET = 50  # skip datasets if len(data) < N\n",
    "\n",
    "# Bootstrapping\n",
    "N_BOOTSTRAPS = 1\n",
    "BOOTSTRAP_SAMPLE_RATIO = 1\n",
    "\n",
    "# Output\n",
    "VERBOSE = False\n",
    "\n",
    "## IGNORE THIS ONE\n",
    "HERE = _dh[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö† From here on, you should _not_ need to modify anything else ü§û\n",
    "\n",
    "---\n",
    "\n",
    "Define key paths for data and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "HERE = Path(HERE)\n",
    "\n",
    "for parent in HERE.parents:\n",
    "    if next(parent.glob(\".github/\"), None):\n",
    "        REPO = parent\n",
    "        break\n",
    "\n",
    "FEATURES_STORE = REPO / \"features\"\n",
    "        \n",
    "OUT = HERE / \"_output\" / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"This notebook:           HERE = {HERE}\")\n",
    "print(f\"This repo:               REPO = {REPO}\")\n",
    "print(f\"Features:      FEATURES_STORE = {FEATURES_STORE}\")\n",
    "print(f\"Outputs in:               OUT = {OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasty trick: save all-caps local variables (CONSTANTS working as hyperparameters) so far in a dict to save it later\n",
    "_hparams = {key: value for key, value in locals().items() if key.upper() == key and not key.startswith((\"_\", \"OE_\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from kinoml.utils import seed_everything, import_object\n",
    "from kinoml.core import measurements as measurement_types\n",
    "from kinoml.datasets.torch_datasets import AwkwardArrayDataset\n",
    "from kinoml.core.measurements import null_observation_model\n",
    "\n",
    "# Fix the seed for reproducible random splits -- otherwise we get mixed train/test groups every time, biasing the model evaluation\n",
    "seed_everything();\n",
    "print(\"Run started at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load featurized data and create observation models\n",
    "\n",
    "We assume this path structure: `$REPO/features/_output/<FEATURIZATION>/<DATASET>/<MEASUREMENT_TYPE>.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = []\n",
    "MEASUREMENT_TYPES = set()\n",
    "for glob in PARQUET_FILES:\n",
    "    parquets = list(FEATURES_STORE.glob(glob))\n",
    "    if not parquets:\n",
    "        warn(f\"‚ö† NPZ glob `{glob}` did not match any files!\")\n",
    "        continue\n",
    "        \n",
    "    for parquet in parquets:\n",
    "        measurement_type = parquet.stem\n",
    "        dataset = parquet.parent.name\n",
    "        \n",
    "        ds = AwkwardArrayDataset.from_parquet(parquet)\n",
    "        ds.metadata = {\n",
    "            \"dataset\": dataset,\n",
    "            \"measurement_type\": measurement_type,\n",
    "        }\n",
    "        DATASETS.append(ds)\n",
    "        MEASUREMENT_TYPES.add(measurement_type)\n",
    "#         if not VALIDATION:\n",
    "#             ds.indices[\"test\"] = np.concatenate([ds.indices[\"test\"], ds.indices[\"val\"]])\n",
    "#             ds.indices[\"val\"] = np.array([])\n",
    "\n",
    "if not DATASETS:\n",
    "    raise ValueError(\"Provided `PARQUET_FILES` did not result in any valid datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data-dependent objects, we can start with the model-specific definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(DATASETS)} datasets with a total of {len(MEASUREMENT_TYPES)} measurement types:\", \", \".join(sorted(MEASUREMENT_TYPES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.ml.lightning_modules import KFold3Way, KFold\n",
    "from IPython.display import Markdown\n",
    "from tqdm.auto import trange, tqdm\n",
    "from kinoml.ml.torch_models import NeuralNetworkRegression\n",
    "from ipywidgets import HBox, VBox, Output, HTML\n",
    "from kinoml.analysis.plots import predicted_vs_observed, performance\n",
    "from kinoml.utils import fill_until_next_multiple\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "if VALIDATION:\n",
    "    kfold = KFold3Way(n_splits=N_SPLITS, shuffle=SHUFFLE_FOLDS)\n",
    "    ttypes = [\"train\", \"val\", \"test\"]\n",
    "else:\n",
    "    kfold = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE_FOLDS)\n",
    "    ttypes = [\"train\", \"test\"]\n",
    "\n",
    "ModelCls = import_object(MODEL_CLS)\n",
    "    \n",
    "kinase_metrics = defaultdict(dict)\n",
    "for dataset in tqdm(DATASETS):\n",
    "    name = dataset.metadata[\"measurement_type\"]\n",
    "    mtype = import_object(f\"kinoml.core.measurements.{name}\")\n",
    "    if len(dataset) < MIN_ITEMS_PER_DATASET:\n",
    "        warn(f\"Ignoring {name} because it has less than {MIN_ITEMS_PER_DATASET}\")\n",
    "        continue\n",
    "            \n",
    "    if VERBOSE:\n",
    "        display(Markdown(f\"#### {name}\"))\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    ds_size = list(range(len(dataset)))\n",
    "    for fold_index, splits in enumerate(kfold.split(ds_size, ds_size)):\n",
    "        if VALIDATION:\n",
    "            train_indices, val_indices, test_indices = splits\n",
    "        else:\n",
    "            train_indices, test_indices = splits\n",
    "\n",
    "        if VERBOSE:\n",
    "            display(Markdown(f\"##### Fold {fold_index}\"))\n",
    "\n",
    "        ####\n",
    "        # TRAIN\n",
    "        ####\n",
    "        \n",
    "        x_train, y_train = dataset[train_indices]\n",
    "        x_test, y_test = dataset[test_indices]\n",
    "        \n",
    "        if VALIDATION:\n",
    "            x_val, y_val = dataset[val_indices]\n",
    "        \n",
    "        if ModelCls.needs_input_shape:\n",
    "            MODEL_KWARGS[\"input_shape\"] = ModelCls.estimate_input_shape(x_train)\n",
    "        nn_model = ModelCls(**MODEL_KWARGS)\n",
    "        nn_model.train(True)\n",
    "\n",
    "        optimizer = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE, eps=EPSILON, betas=BETAS)\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "        loss_adapter = mtype.loss_adapter(backend=\"pytorch\")\n",
    "        \n",
    "        if VERBOSE:\n",
    "            range_epochs = trange(MAX_EPOCHS, desc=\"Epochs (+ featurization...)\")\n",
    "        else:\n",
    "            range_epochs = range(MAX_EPOCHS)\n",
    "        for epoch in range_epochs:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = nn_model(x_train).view_as(y_train)\n",
    "            loss = loss_adapter(prediction, y_train, loss_func=loss_function)\n",
    "            \n",
    "            if VERBOSE:\n",
    "                range_epochs.set_description(f\"Epochs (loss={loss.item():.2e})\")\n",
    "\n",
    "            if VALIDATION:\n",
    "                warn(\"Validation step not implemented yet\")\n",
    "\n",
    "\n",
    "            # Gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer.step()\n",
    "        \n",
    "        ###\n",
    "        # Save model's state -- you will still need to instantiate the model class!\n",
    "        # Possibly using something like:\n",
    "        # model = import_object(MODEL_CLS)(**MODEL_KWARGS)\n",
    "        # model.load_state_dict(torch.load(\"state_dict.pt\"))\n",
    "        ###\n",
    "        torch.save(nn_model.state_dict(), OUT / f\"state_dict_{name}_fold{fold_index}.pt\")\n",
    "        \n",
    "        ####\n",
    "        # EVAL\n",
    "        ####\n",
    "        nn_model.eval()\n",
    "        outputs = []\n",
    "        obs_model = mtype.observation_model(backend=\"pytorch\")\n",
    "        for ttype in ttypes:\n",
    "            output = Output()\n",
    "            with output:\n",
    "                title = f\"fold={fold_index}, {ttype}={locals()[f'{ttype}_indices'].shape[0]}\"\n",
    "                print(title)\n",
    "                print(\"-\"*(len(title)))\n",
    "\n",
    "                observed = locals()[f\"y_{ttype}\"]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predicted = nn_model(locals()[f\"x_{ttype}\"])\n",
    "                    predicted = obs_model(predicted)\n",
    "\n",
    "                predicted = predicted.view_as(observed).detach().numpy()\n",
    "                observed = observed.detach().numpy()\n",
    "                these_metrics = performance(predicted, observed, n_boot=N_BOOTSTRAPS, sample_ratio=BOOTSTRAP_SAMPLE_RATIO)\n",
    "                metrics[ttype].append(these_metrics)\n",
    "                if VERBOSE:\n",
    "                    display(predicted_vs_observed(predicted, observed, mtype_class, with_metrics=False))\n",
    "\n",
    "            outputs.append(output)\n",
    "        if VERBOSE:\n",
    "            display(HBox(outputs))\n",
    "\n",
    "    # Average performances\n",
    "\n",
    "    average = defaultdict(dict)\n",
    "    for key in metrics[\"test\"][0]:\n",
    "        for label in ttypes:\n",
    "            # this zero here ---v is super important! we only want the mean of the means!\n",
    "            values =  [fold[key][0] for fold in metrics[label]]\n",
    "            average[label][key] = {\n",
    "                \"mean\": np.mean(values),\n",
    "                \"std\": np.std(values)\n",
    "            }\n",
    "    if VERBOSE:\n",
    "        for label in ttypes:    \n",
    "            display(HTML(f\"Bootstrapped average across folds ({label}):\"))\n",
    "            display(pd.DataFrame.from_dict(average[label]))\n",
    "    kinase_metrics[name] = average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "`kinase_metrics` is a nested dictionary with these dimensions:\n",
    "\n",
    "- kinase name\n",
    "- measurement type\n",
    "- metric\n",
    "- mean & standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Configuration \n",
    "\n",
    "```json\n",
    "{json.dumps(_hparams, default=str, indent=2)}\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "if VERBOSE:\n",
    "    display(Markdown(f\"\"\"\n",
    "\n",
    "    ### Kinase metrics\n",
    "\n",
    "    ```json\n",
    "    {json.dumps(kinase_metrics, default=str, indent=2)}\n",
    "    ```\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mtype_name in MEASUREMENT_TYPES:\n",
    "    mtype_metrics = metrics.get(mtype_name)\n",
    "    if not mtype_metrics:\n",
    "        continue\n",
    "        \n",
    "    display(Markdown(f\"#### {mtype_name}\"))\n",
    "    \n",
    "    # flatten dict a bit: from dict[\"test\"][\"r2\"][\"mean\"] to dict[\"test\"][\"r2_mean\"]\n",
    "    flattened = {}\n",
    "    for ttype, scores in mtype_metrics.items():\n",
    "        flattened[ttype] = {}\n",
    "        for score, stats in scores.items():\n",
    "            for stat, value in stats.items():\n",
    "                flattened[ttype][f\"{score}_{stat}\"] = value\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(flattened, orient=\"index\")\n",
    "    with pd.option_context(\"display.float_format\", \"{:.3f}\".format, \"display.max_rows\", len(df)):\n",
    "        display(\n",
    "            df.style.background_gradient(subset=[\"r2_mean\"], low=0, high=1, vmin=0, vmax=1)\n",
    "              .apply(lambda x: ['font-weight: bold' for v in x], subset=[\"r2_mean\", \"r2_std\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run finished at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save reports to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.utils import watermark\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(OUT / \"performance.json\", \"w\") as f:\n",
    "    json.dump(kinase_metrics, f, default=str, indent=2)\n",
    "    \n",
    "with open(OUT/ \"watermark.txt\", \"w\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "with open(OUT / \"hparams.json\", \"w\") as f:\n",
    "    json.dump(_hparams, f, default=str, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
