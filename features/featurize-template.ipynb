{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize a dataset\n",
    "\n",
    "Any machine learning model will expect tensorial representations of the chemical data. This notebooks provides a workflow to achieve such goal.\n",
    "\n",
    "`kinoml.dataset.DatasetProvider` objects need to be available to deal with your collection of raw measurements for protein:ligand systems. These objects are, roughly, a list of `kinoml.core.BaseMeasurement`, each containing a set of `.values` and a some extra metadata, like the `system` objects to be featurized here.\n",
    "\n",
    "In ligand-based models, protein information is only considered marginally, and most of the action happens at the ligand level. Usually starting with a string representation such as SMILES, or a database identifier such as a PubChem ID, these are promoted to (usually) RDKit objects and then transformed into a tensor of some form (e.g. fingerprints, molecular graph as an adjacency matrix, etc).\n",
    "\n",
    "Available featurizers can be found under `kinoml.features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Run `python run_notebook.py --help` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is the template file (and not a copy) and you are introducing changes,\n",
    "# update VERSION with the current date (YYYY.MM.DD)\n",
    "VERSION = \"2021.05.18\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# TEMPLATE VALUES -- these are overriden (see below if executed) by papermill using a YAML or Python file as input\n",
    "DATASET_CLS = \"import.path.to.DatasetProvider\"\n",
    "DATASET_KWARGS = {\"option\": \"value\", \"option2\": \"value2\"}\n",
    "\n",
    "FEATURIZE_KWARGS = {}\n",
    "\n",
    "PIPELINES = {\n",
    "    \"someuniquekey\": [\n",
    "        (\"import.path.to.SomeFeaturizer\", {\"option\": \"value\", \"option2\": \"value2\"}),\n",
    "        (\"import.path.to.SomeOtherFeaturizer\", {\"option\": \"value\", \"option2\": \"value2\"}),\n",
    "    ]\n",
    "}\n",
    "PIPELINES_AGG = \"kinoml.features.core.Concatenated\"\n",
    "PIPELINES_AGG_KWARGS = {}\n",
    "\n",
    "GROUPS = [\n",
    "    (\"kinoml.datasets.groups.CallableGrouper\", {\"function\": \"lambda something: something.attribute\"}),\n",
    "    (\"kinoml.datasets.groups.CallableGrouper\", {\"function\": \"lambda otherthing: otherthing.attribute2\"})\n",
    "]\n",
    "\n",
    "TRAIN_TEST_VAL_KWARGS = {\"idx_train\": 0.8, \"idx_test\": 0.1, \"idx_val\": 0.1}\n",
    "\n",
    "## IGNORE THIS ONE\n",
    "HERE = _dh[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš  From here on, you should _not_ need to modify anything else ðŸ¤ž\n",
    "\n",
    "---\n",
    "\n",
    "Define key paths for data and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HERE = Path(HERE)\n",
    "for parent in HERE.parents:\n",
    "    if next(parent.glob(\".github/\"), None):\n",
    "        REPO = parent\n",
    "        break\n",
    "\n",
    "# Generate paths for this pipeline\n",
    "featurizer_path = []\n",
    "for name, branch in PIPELINES.items():\n",
    "    featurizer_path.append(name)\n",
    "    for clsname, kwargs in branch:\n",
    "        clsname = clsname.rsplit(\".\", 1)[1]\n",
    "        kwargs = [f\"{k}={''.join(c for c in str(v) if c.isalnum())}\" for k,v in kwargs.items()]\n",
    "        featurizer_path.append(\"_\".join([clsname] + kwargs))\n",
    "\n",
    "OUT = HERE / \"_output\"  / \"__\".join(featurizer_path) / DATASET_CLS.rsplit('.', 1)[1]\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"This notebook:           HERE = {HERE}\")\n",
    "print(f\"This repo:               REPO = {REPO}\")\n",
    "print(f\"Outputs in:               OUT = {OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasty trick: save all-caps local variables (CONSTANTS working as hyperparametrs) so far in a dict to save it later\n",
    "_hparams = {key: value for key, value in locals().items() if key.upper() == key and not key.startswith((\"_\", \"OE_\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup is finished, start working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "from kinoml.utils import seed_everything, import_object\n",
    "seed_everything();\n",
    "print(\"Run started at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data\n",
    "\n",
    "> This `import_object` function allows us to take a `str` containing a Python import path (e.g. `kinoml.datasets.chembl.ChEMBLDatasetProvider`) and obtain the imported object directly. That's how we can encode classes in JSON-only `papermill` inputs.\n",
    ">\n",
    "> See the help message `import_object?` for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = import_object(DATASET_CLS).from_source(**DATASET_KWARGS)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "from kinoml.features.core import Pipeline\n",
    "\n",
    "pipelines = []\n",
    "for key, pipeline_instructions in PIPELINES.items():\n",
    "    print(f\"Building featurizer `{key}` with instructions:\")\n",
    "    featurizers = []\n",
    "    for featurizer_import_str, kwargs in pipeline_instructions:\n",
    "        kwargs = kwargs or {}  # make sure empty values (None, \"\") turn into {} so we can do **kwargs below\n",
    "        print(f\"  Instantiating `{featurizer_import_str}` with options `{kwargs}`\")\n",
    "        featurizers.append(import_object(featurizer_import_str)(**kwargs))\n",
    "    pipelines.append(Pipeline(featurizers))\n",
    "print(\"Resulting pipelines:\", *pipelines)\n",
    "aggregated_pipeline = import_object(PIPELINES_AGG)(pipelines, **PIPELINES_AGG_KWARGS)\n",
    "print(\"Aggregated pipelines:\", aggregated_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefeaturize everything\n",
    "aggregated_pipeline.featurize(dataset.systems, **FEATURIZE_KWARGS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove systems that couldn't be featurized. Successful featurizations are stored in `measurement.system.featurizations['last']` so we test for that key existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.datasets.groups import CallableGrouper, RandomGrouper\n",
    "grouper = CallableGrouper(lambda measurement: 'invalid' if 'last' not in measurement.system.featurizations else 'valid')\n",
    "grouper.assign(dataset, overwrite=True, progress=False)\n",
    "groups = dataset.split_by_groups()\n",
    "if \"invalid\" in groups:\n",
    "    _invalid = groups.pop(\"invalid\")\n",
    "    warn(f\"{len(_invalid)} entries could not be featurized!. Possible errors:\")\n",
    "    warn(f\"{_invalid[0].system.featurizations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups\n",
    "\n",
    "Cumulatively apply groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[(\"valid\",)] = groups.pop(\"valid\")\n",
    "if GROUPS:\n",
    "    for grouper_str, grouper_kwargs in GROUPS:\n",
    "        grouper_cls = import_object(grouper_str)\n",
    "        ## We need this because lambda functions are not JSON-serializable\n",
    "        if issubclass(grouper_cls, CallableGrouper):\n",
    "            for k, v in list(grouper_kwargs.items()):\n",
    "                if k == \"function\" and isinstance(v, str):\n",
    "                    grouper_kwargs[k] = eval(v)  # sorry :)\n",
    "        ## End of lambda hack\n",
    "        grouper = grouper_cls(**grouper_kwargs)        \n",
    "        for group_key in list(groups.keys()):\n",
    "            grouper.assign(groups[group_key], overwrite=True, progress=False)\n",
    "            for subkey, subgroup in groups.pop(group_key).split_by_groups().items():\n",
    "                groups[group_key + (subkey,)] = subgroup\n",
    "print(\"10 groups to show keys:\", *list(groups.keys())[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write tensors to disk\n",
    "\n",
    "Output files are written to `_output/<PIPELINE>/<DATASET>/<GROUP>.parquet` files.\n",
    "\n",
    "Each `parquet` will contain at least two array-like objects. The dimensionality of the parquet files is built as `(systems, X_or_y, ...)`. For example, the first X vector for the first system is accessed like `parquet[0, \"0\"]`. Notice how the 2nd index is a string! (`awkward` design).\n",
    "\n",
    "- `\"0\"` (X, featurized systems). See `DatasetProvider.to_awkward` for more info.\n",
    "- `\"1\"` (y, associated measurements)\n",
    "\n",
    "If `X` is composed of more than one array (e.g. connectivity matrix + node features), these are flattened to `\"0\"`, `\"1\"`, `\"2\"`, and so on. `y` is ALWAYS the last one in that list (accessible via `data.fields`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grouper = RandomGrouper(TRAIN_TEST_VAL_KWARGS)\n",
    "\n",
    "parquets = []\n",
    "for group, ds in sorted(groups.items(), key=lambda kv: len(kv[1]), reverse=True):\n",
    "    indices = random_grouper.indices(ds)\n",
    "    X, y = ds.to_awkward()\n",
    "    parquet = ak.zip([*X, y], depth_limit=1)\n",
    "    path = OUT / f\"{'__'.join([g for g in group if g != 'valid'])}.parquet\"\n",
    "    parquets.append(path)\n",
    "    ak.to_parquet(parquet, path)\n",
    "    # TODO: Missing indices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview generated Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.datasets.torch_datasets import AwkwardArrayDataset\n",
    "awk = AwkwardArrayDataset.from_parquet(parquets[0])\n",
    "awk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = awk[0]  # (multi-X) and y tensors for first system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run finished at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free some memory first\n",
    "del awk, parquets, groups, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.utils import watermark\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(OUT/ \"watermark.txt\", \"w\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "with open(OUT / \"hparams.json\", \"w\") as f:\n",
    "    json.dump(_hparams, f, default=str, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
