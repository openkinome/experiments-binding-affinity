{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize a dataset\n",
    "\n",
    "Any machine learning model will expect tensorial representations of the chemical data. This notebooks provides a workflow to achieve such goal.\n",
    "\n",
    "`kinoml.dataset.DatasetProvider` objects need to be available to deal with your collection of raw measurements for protein:ligand systems. These objects are, roughly, a list of `kinoml.core.BaseMeasurement`, each containing a set of `.values` and a some extra metadata, like the `system` objects to be featurized here.\n",
    "\n",
    "In ligand-based models, protein information is only considered marginally, and most of the action happens at the ligand level. Usually starting with a string representation such as SMILES, or a database identifier such as a PubChem ID, these are promoted to (usually) RDKit objects and then transformed into a tensor of some form (e.g. fingerprints, molecular graph as an adjacency matrix, etc).\n",
    "\n",
    "Available featurizers can be found under `kinoml.features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Run `python run_notebook.py --help` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is the template file (and not a copy) and you are introducing changes,\n",
    "# update VERSION with the current date (YYYY.MM.DD)\n",
    "VERSION = \"2020.12.07\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úè Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# TEMPLATE VALUES -- these are overriden (see below if executed) by papermill using a YAML or Python file as input\n",
    "DATASET_CLS = \"import.path.to.DatasetProvider\"\n",
    "DATASET_KWARGS = {\"option\": \"value\", \"option2\": \"value2\"}\n",
    "\n",
    "PIPELINE = {\n",
    "    \"someuniquekey\": [\n",
    "        (\"import.path.to.SomeFeaturizer\", {\"option\": \"value\", \"option2\": \"value2\"}),\n",
    "        (\"import.path.to.SomeOtherFeaturizer\", {\"option\": \"value\", \"option2\": \"value2\"}),\n",
    "    ]\n",
    "}\n",
    "PIPELINE_AGG = \"kinoml.features.core.Concatenated\"\n",
    "PIPELINE_AGG_KWARGS = {}\n",
    "\n",
    "FEATURIZE_KWARGS = {\"processes\": 1}\n",
    "\n",
    "GROUPS = [\n",
    "    (\"kinoml.datasets.groups.CallableGrouper\", {\"function\": \"lambda something: something.attribute\"}),\n",
    "    (\"kinoml.datasets.groups.CallableGrouper\", {\"function\": \"lambda otherthing: otherthing.attribute2\"})\n",
    "]\n",
    "\n",
    "TRAIN_TEST_VAL_KWARGS = {\"idx_train\": 0.8, \"idx_test\": 0.1, \"idx_val\": 0.1}\n",
    "\n",
    "## IGNORE THIS ONE\n",
    "HERE = _dh[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö† From here on, you should _not_ need to modify anything else ü§û\n",
    "\n",
    "---\n",
    "\n",
    "Define key paths for data and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "HERE = Path(HERE)\n",
    "for parent in HERE.parents:\n",
    "    if next(parent.glob(\".github/\"), None):\n",
    "        REPO = parent\n",
    "        break\n",
    "\n",
    "# Generate paths for this pipeline\n",
    "featurizer_path = []\n",
    "for name, branch in PIPELINE.items():\n",
    "    featurizer_path.append(name)\n",
    "    for clsname, kwargs in branch:\n",
    "        clsname = clsname.rsplit(\".\", 1)[1]\n",
    "        kwargs = [f\"{k}={''.join(c for c in str(v) if c.isalnum())}\" for k,v in kwargs.items()]\n",
    "        featurizer_path.append(\"_\".join([clsname] + kwargs))\n",
    "\n",
    "OUT = HERE / \"_output\"  / \"__\".join(featurizer_path) / DATASET_CLS.rsplit('.', 1)[1]\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"This notebook:           HERE = ~/{HERE.relative_to(Path.home())}\")\n",
    "print(f\"This repo:               REPO = ~/{REPO.relative_to(Path.home())}\")\n",
    "print(f\"Outputs in:               OUT = ~/{OUT.relative_to(Path.home())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasty trick: save all-caps local variables (CONSTANTS working as hyperparametrs) so far in a dict to save it later\n",
    "_hparams = {key: value for key, value in locals().items() if key.upper() == key and not key.startswith((\"_\", \"OE_\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup is finished, start working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out some warnings thrown by openforcefield and rdkit\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from kinoml.utils import seed_everything, import_object\n",
    "seed_everything();\n",
    "print(\"Run started at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = import_object(DATASET_CLS).from_source(**DATASET_KWARGS)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "from kinoml.features.core import Pipeline\n",
    "\n",
    "featurizers = []\n",
    "for key, featurizer_instructions in PIPELINE.items():\n",
    "    featurizers.append(Pipeline([import_object(import_str)(**kwargs) for import_str, kwargs in featurizer_instructions]))\n",
    "featurizer = import_object(PIPELINE_AGG)(featurizers, **PIPELINE_AGG_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefeaturize everything\n",
    "dataset.featurize(featurizer, **FEATURIZE_KWARGS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove systems that couldn't be featurized. Successful featurizations are stored in `measurement.system.featurizations['last']` so we test for that key existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.datasets.groups import CallableGrouper, RandomGrouper\n",
    "grouper = CallableGrouper(lambda measurement: 'invalid' if 'last' not in measurement.system.featurizations else 'valid')\n",
    "grouper.assign(dataset, overwrite=True, progress=False)\n",
    "groups = dataset.split_by_groups()\n",
    "if \"invalid\" in groups:\n",
    "    _invalid = groups.pop(\"invalid\")\n",
    "    print(len(_invalid.measurements), \"entries could not be featurized!\", file=sys.stderr)\n",
    "    print(_invalid.measurements[0].system.featurizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups\n",
    "\n",
    "Cumulatively apply groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[(\"valid\",)] = groups.pop(\"valid\")\n",
    "for grouper_str, grouper_kwargs in GROUPS:\n",
    "    grouper_cls = import_object(grouper_str)\n",
    "    ## We need this because lambda functions are not JSON-serializable\n",
    "    if issubclass(grouper_cls, CallableGrouper):\n",
    "        for k, v in list(grouper_kwargs.items()):\n",
    "            if k == \"function\" and isinstance(v, str):\n",
    "                grouper_kwargs[k] = eval(v)  # sorry :)\n",
    "    ## End of lambda hack\n",
    "    grouper = grouper_cls(**grouper_kwargs)        \n",
    "    for group_key in list(groups.keys()):\n",
    "        grouper.assign(groups[group_key], overwrite=True, progress=False)\n",
    "        for subkey, subgroup in groups.pop(group_key).split_by_groups().items():\n",
    "            groups[group_key + (subkey,)] = subgroup\n",
    "print(\"10 groups to show keys:\", *list(groups.keys())[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write tensors to disk\n",
    "\n",
    "Output files are written to `_output/<PIPELINE>/<DATASET>/<GROUP>.npz` files.\n",
    "\n",
    "Each `npz` will contain two `np.ndarray` objects: `X` (featurized systems) and `y` (associated measurements), plus the train/test/validation indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grouper = RandomGrouper(TRAIN_TEST_VAL_KWARGS)\n",
    "\n",
    "for group, ds in sorted(groups.items(), key=lambda kv: len(kv[1]), reverse=True):\n",
    "    indices = random_grouper.indices(ds)\n",
    "    X = np.asarray(ds.featurized_systems())\n",
    "    y = ds.measurements_as_array()\n",
    "    np.savez(OUT / f\"{'__'.join([g for g in group if g != 'valid'])}.npz\", X=X, y=y.astype(\"float32\"), **indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run finished at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinoml.utils import watermark\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "w = watermark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(OUT/ \"watermark.txt\", \"w\") as f:\n",
    "    f.write(cap.stdout)\n",
    "\n",
    "with open(OUT / \"hparams.json\", \"w\") as f:\n",
    "    json.dump(_hparams, f, default=str, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
